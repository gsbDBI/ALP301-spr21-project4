---
title: 'Alp 301: Stones2Milestones, Recommendation Systems 1'
output:
  html_document:
    highlight: haddock
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: '2'
date: "April 2021"
---

```{r, message = FALSE, warning = FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse)
pacman::p_load(here)
pacman::p_load(lmtest)
pacman::p_load(glue)
pacman::p_load(broom)
pacman::p_load(ri2)
pacman::p_load(margins)
pacman::p_load(glmnet) 
pacman::p_load(kableExtra)
pacman::p_load(stargazer)
pacman::p_load(knitr)
pacman::p_load(doParallel)
pacman::p_load(corrplot)
pacman::p_load(viridis)

rm(list = ls())

utility_mat<- read_csv("/cloud/project/Tutorials/Datasets/utils_mat_filtered.csv",col_types = cols(.default = col_double()))
story_info<- read_csv("/cloud/project/Tutorials/Datasets/all_story_obs.csv")

print(head(utility_mat))

source("../Utils/initialize_utility_matrix.R", local = knitr::knit_global())

rm(utility_mat)  # this variable is no longer needed
```

In the last tutorial, we learned about basic recommendation algorithms, and applied them to Music and MovieLens datasets. This tutorial utilizes these techniques for S2M data.

We will use our utility matrix with 1087 stories and 11202 children to build item based and user based collaborative filtering models, and run dimensionality reduction algorithms and use them to make recommendations.

We will use the story_info dataset to analyze the recommendations made by our algorithms. The story_info dataframe includes various observables extracted from the story text. These include the following variables.

`level`: The difficulty level of the story

`totpage`: The number of pages

`wordcount`: The number of words

`has_geoarea, has_color, has_fruits, has_vegetables, has_animals, has_sports`:
Binary variables indicating whether the story text includes any words related to geographic areas, colors, fruits, vegetables, animals, or sports.

`geoarea,color,fruits,vegetables,animals,sports`:
If the story contains references to any of the above, these are the extracted matches from the story text for words in these categories.


# Story similarity based recommendations

Let's make recommendations to someone who read story 100. First, let's have a look at what they read so far by examining the story characteristics.

```{r}
source("../Utils/get_story_details.R", local = knitr::knit_global())

story_characteristics<-get_story_details(100)

story_characteristics
```

# User Similarity Based Recommendations:

Now let's make user-based recommendations.
We will start by calculating the user similarity matrix.

```{r, load_recsys_ubcf}
source("../RecSys/recsys_ubcf.R", local = knitr::knit_global())
```

Let's test it on user 100 again.

```{r, eval = FALSE}
# utility_matrix <- Matrix(utility_matrix, sparse=TRUE)
similarity_matrix_user <- get_similarity_matrix_ubcf(utility_matrix)
similarity_matrix_user <- Matrix(similarity_matrix_user, sparse = TRUE)

#Top 5 recommendations to user 100
recs<-ubcf_top_x_recommendations(100,5,utility_matrix,similarity_matrix_user)
print(recs)

#Lets compare story `user_interacted` which is something they've read, vs our top user based recommendation""
user_interacted<-names(utility_matrix[100,utility_matrix[100,]>0])
get_story_details_2(user_interacted[1])
get_story_details_2(recs[1])

```

## Question
Plot the popularity of the overall items and the results of the  (top 1, top 5) user-based recommendations for every user. Is this method recommending more popular items than average?

```{r diversity_user_based_recs_parallel, eval = FALSE}

# Note: This might take a while to run
recommender_results_item_one <- vector()
recommender_results_item_five <- vector()

# Set up parallel processing
all_cores <- min(parallel::detectCores(logical = FALSE), 8)
doFuture::registerDoFuture()
cl <- parallel::makeCluster(all_cores)
future::plan("cluster", workers = cl)

# Export objects to the parallel sessions
parallel::clusterExport(cl, c("utility_matrix", "ubcf_top_x_recommendations", "story_ids", "child_ids", "similarity_matrix_user"))

recommender_results_item_one <- foreach(i = 1:nrow(utility_matrix), .combine = append) %dopar% {
  return(ubcf_top_x_recommendations(i, 1, utility_matrix, similarity_matrix_user))
}

recommender_results_item_five <- foreach(i = 1:nrow(utility_matrix), .combine = append) %dopar% {
  return(ubcf_top_x_recommendations(i, 5, utility_matrix, similarity_matrix_user))
}

parallel::stopCluster(cl)

recommender_results_item_one <- unique(recommender_results_item_one)
recommender_results_item_five <- unique(recommender_results_item_five)

# First, note how many stories in total are recommended, out of 1087:
length(recommender_results_item_one)
length(recommender_results_item_five)
```

```{r diversity_user_based_recs_sequentiall, eval=FALSE}

recommender_results_item_one <- foreach(i = 1:nrow(utility_matrix), .combine = append) %dopar% {
  return(User_top_X_recommendations(i, 1, utility_matrix, similarity_matrix_user))
}

recommender_results_item_five <- foreach(i = 1:10, .combine = append) %dopar% {
  return(User_top_X_recommendations(i, 5, utility_matrix, similarity_matrix_user))
}

recommender_results_item_one <- unique(recommender_results_item_one)
recommender_results_item_five <- unique(recommender_results_item_five)

# First, note how many stories in total are recommended, out of 1087:
length(recommender_results_item_one)
length(recommender_results_item_five)
```

```{r, eval=FALSE}
source("../Utils/save_recommendation_results.R", local = knitr::knit_global())
recommender_fun_args <- list(X=10, ratings_matrix=utility_matrix, similarity_matrix_user=similarity_matrix_user)
Save_Recommender_Results(User_top_X_recommendations, recommender_fun_args, "rec_results_ubcf.csv", full=FALSE)
```

```{r, eval=FALSE}
recommender_results_item <- tibble(read_csv("../Results/rec_results_ubcf.csv"))

recommender_results_item_one <- unique(filter(recommender_results_item, item_rank <= 1)$story_id)
recommender_results_item_five <- unique(filter(recommender_results_item, item_rank <= 5)$story_id)
```

```{r plot_user_based_recs, eval=FALSE}
index_results_one <- which(recommender_results_item_one %in% story_ids)
index_results_five <- which(recommender_results_item_five %in% story_ids)

popularity_ones <- colSums(utility_matrix[, index_results_one]>0.3)
popularity_fives <- colSums(utility_matrix[, index_results_five]>0.3)

readers_per_story <- (colSums(utility_matrix>0.3))

# Create a data frame of recommendations
df1 <- data.frame(x = readers_per_story, label = rep("Overall", length(readers_per_story)))
df2 <- data.frame(x = popularity_fives, label = rep("Five_Recs", length(popularity_fives)))
df3 <- data.frame(x = popularity_ones, label = rep("One_Rec", length(popularity_ones)))
df <- rbind(df1, df2, df3)

# Plot the number of readers per story
ggplot(df, aes(x, y = ..count.., fill = label)) +
  geom_density(color = "black", alpha = 0.5) +
  xlab("Number of readers") +
  ylab("Density") + 
  xlim(0, 1000)

```


# Evaluating collaborative filtering models

$$
\textit{Precision at N}=Mean\left(\frac{\textit{Recommended and used}}{N}\right)
$$

$$
\textit{Recall at N}=Mean\left(\frac{\textit{Recommended and used}}{\textit{Total items used by user}}\right)
$$

$$
\textit{Root Mean Squared Error:} \sqrt{Average((Predicted Rating-Actual Rating)^2)}
$$

Let us now evaluate our algorithms. When calculating precision and recall, each user in the test set is taken as the current 'active user', and a random subset of their existing history is hidden. This means we will randomly remove part of the artists they listen to, and see how well our recommendations match with the held out data. There are various ways of doing this, and here we will use 'all but k'. For example, in "all but 1", if the user listened to 10 artists, we will use 9 of those to predict the 10th, or in "all but 5" we will use 5 of those to predict the remaining 5. Note that when evaluating performance using the 'all but k' approach, we can only use data points which have more than 'k' items. When calculating RMSE, we calculate the predicted score for every item, and calculate the squared difference between the predicted score and the actual ratings for every item and every user, and then rake the average.


```{r UBCF}
#This is not run by default,
#because it takes a long time.
#Delete "eval=FALSE" in rmarkdown
#and run it yourself.

#We will evaluate it in the first 50 users, to save time,
#because matrix operations take a long time to compute. 
#Note that we need to re-calculate the similarity 
#matrix for every user when we hide 
#part of their history. 
#Try running this for the full dataset on your own.
test_all_but_k_users<-function(k,ratings_matrix){
  precision_vector<-vector()
  recall_vector<-vector()
  rmse_vector<-vector()
  #Change the line below to run it on the entire dataset
  num_rows=min(50,nrow(ratings_matrix))
  for (userid in 1:num_rows){
    user=ratings_matrix[userid,]
    index_of_known<-which(user!=0)
    if(length(index_of_known)>k){
      takeout<-sample(1:length(index_of_known),size=k)
      stories_removed<-index_of_known[takeout]
      save_old<-user[stories_removed]
      ratings_matrix[userid,stories_removed]<-0
      sim_matrix_user <- get_similarity_matrix_ubcf(ratings_matrix)
      sim_matrix_user = Matrix(sim_matrix_user, sparse = TRUE)
      recommended<-ubcf_top_x_recommendations(userid,k,ratings_matrix,sim_matrix_user)
      matched=length(intersect(story_ids[stories_removed],recommended))
      precision_vector<-append(precision_vector,matched/k)
      recall_vector<-append(recall_vector,(matched/(length(index_of_known))))
      ratings_matrix[userid,stories_removed]<-save_old
      allscores<-get_all_item_scores_ubcf(userid,ratings_matrix,sim_matrix_user)
      rmse<-sqrt(mean((allscores-ratings_matrix[userid,])^2))
      rmse_vector<-append(rmse_vector,rmse)
    }
  }
  return(c("Precision"=mean(precision_vector),"Recall"=mean(recall_vector), "RMSE"=mean(rmse_vector)))
}
#Performance of user based recommendations
# utility_matrix <- Matrix(utility_matrix, sparse = TRUE)
test_all_but_k_users(1,utility_matrix)
test_all_but_k_users(5,utility_matrix)
test_all_but_k_users(10,utility_matrix)
```



## Questions

Why do the testing functions give you three separate values for RMSE? Why are they increasing?

Summarize the activity of users: How many stories they interact with,open, and finish, using the examples from the data descriptives tutorial. Compare the performance of item based and user based collaborative filtering on those with +30 stories interacted, versus those with fewer stories interacted. Which method performs better on less active users? 

Combining the utility matrix with the user_info dataset, compare the performance of collaborative filtering methods on older children (grade 4 or above) versus younger children. Do you see any patterns?
