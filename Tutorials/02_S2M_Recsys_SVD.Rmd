---
title: "02_S2M_Revsys_SVD"
output:
  html_document:
    highlight: haddock
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: '2'
date: "April 2021"
---

```{r, message = FALSE, warning = FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse)
pacman::p_load(here)
pacman::p_load(lmtest)
pacman::p_load(glue)
pacman::p_load(broom)
pacman::p_load(ri2)
pacman::p_load(margins)
pacman::p_load(glmnet) 
pacman::p_load(kableExtra)
pacman::p_load(stargazer)
pacman::p_load(knitr)
pacman::p_load(doParallel)
pacman::p_load(corrplot)
pacman::p_load(viridis)

rm(list = ls())

story_info<- read_csv("Datasets/all_story_obs.csv")
utility_mat<- read_csv("Datasets/utils_mat_filtered.csv",col_types = cols(.default = col_double()))
num_cols <- 2527

child_ids<-as.integer(utility_mat$child_id_code)
story_ids<-as.integer(colnames(utility_mat[,3:num_cols]))
utility_mat[is.na(utility_mat)]<-0.0

stories_with_text<-(story_ids %in% story_info$story_id_code)
utility_matrix<-utility_mat[,3:num_cols]
utility_matrix<-utility_matrix[,stories_with_text]
story_ids<-colnames(utility_matrix)
utility_matrix<-as.matrix(utility_matrix)

rm(utility_mat)  # this variable is no longer needed
```

In the last tutorial, we learned about basic recommendation algorithms, and applied them to Music and MovieLens datasets. This tutorial utilizes these techniques for S2M data.

We will use our utility matrix with 1087 stories and 11202 children to build item based and user based collaborative filtering models, and run dimensionality reduction algorithms and use them to make recommendations.

We will use the story_info dataset to analyze the recommendations made by our algorithms. The story_info dataframe includes various observables extracted from the story text. These include the following variables.

`level`: The difficulty level of the story

`totpage`: The number of pages

`wordcount`: The number of words

`has_geoarea, has_color, has_fruits, has_vegetables, has_animals, has_sports`:
Binary variables indicating whether the story text includes any words related to geographic areas, colors, fruits, vegetables, animals, or sports.

`geoarea,color,fruits,vegetables,animals,sports`:
If the story contains references to any of the above, these are the extracted matches from the story text for words in these categories.

Let's make recommendations to someone who read story 100. First, let's have a look at what they read so far by examining the story characteristics.

```{r}
#From index in the matrix
get_story_details<-function(story_index){
  story_id<-story_ids[story_index]
  story_info%>%filter(story_id_code==story_id)->story_characteristics
  return(story_characteristics)
}

#From 'story text file' id
get_story_details_2<-function(story_name){
  story_info%>%filter(story_id_code==story_name)->story_characteristics
  return(story_characteristics)
}

story_characteristics<-get_story_details(100)

story_characteristics
```

# Matrix Factorization and Dimensionality Reduction

Let's create a lower dimensional representation of our utility matrix.

```{r}

#Let's start with a 5-dimensional representation
d<-5
U<-svd(utility_matrix,nu=d,nv=d)$u
Vprime<-svd(utility_matrix,nu=d,nv=d)$v

```

```{r}
#Top recommendations based on low dimensional representation:

SVD_top_X_recommendations<-function(U,Vprime,userid,X,ratings_matrix){
  user_vector<-U[userid,]
  scores<-U[userid,]%*%t(Vprime)
  user_row<-ratings_matrix[userid,]
  unknown_stories<-user_row==0
  names_unknown<-story_ids[unknown_stories]
  index <- which(scores[as.logical(unknown_stories)] >= sort(scores[as.logical(unknown_stories)], decreasing=T)[X], arr.ind=TRUE)
  return(names_unknown[index])
}

SVD_item_scores<-function(U,Vprime,userid){
  user_vector<-U[userid,]
  scores<-U[userid,]%*%t(Vprime)
  return(scores)
}
```

Let us make recommendations to user 100 again and compare against a story they've read.

```{r}

toprec<-SVD_top_X_recommendations(U,Vprime,100,1,utility_matrix)

#Lets compare the content of story `user_interacted` which is something they've read, against our top SVD recommendation
#What does user 100 read?
user_interacted<-names(utility_matrix[100,utility_matrix[100,]>0])
get_story_details_2(user_interacted[1])
get_story_details_2(as.numeric(toprec))

```

```{r, eval=FALSE}
source("Utils/save_recomendation_results.R", local = knitr::knit_global())
recommender_fun_args <- list(U=U,Vprime=Vprime,X=10, ratings_matrix=utility_matrix)
Save_Recommender_Results(SVD_top_X_recommendations, recommender_fun_args, "rec_results_svc.csv", full=TRUE)
```

## Cross Validation for SVD


How do we choose how many dimensions to use? It's a good idea to do cross-validation, using precision at K as our metric.

```{r, eval=True}
#This is not run by default,
#because it takes a long time.
#Delete "eval=FALSE" in rmarkdown
#and run it yourself.

# Parallelize computing to make things faster
cluster <- parallel::makeCluster(min(parallel::detectCores(logical = TRUE) - 1, 8)) 
registerDoParallel(cluster)
parallel::clusterEvalQ(cluster, {})

folds <- 5
splitfolds <- sample(1:folds, nrow(utility_matrix), replace = TRUE)
candidate_d <- c(2,4,6,8,10,12,14,16,18,20) #Candidate dimensions
k<-5 # We will use precision at 5

# Export objects to the parallel sessions
parallel::clusterExport(cluster, c("utility_matrix", "splitfolds", "folds", "candidate_d","SVD_top_X_recommendations","k","story_ids"))

system.time({
results <- foreach(j = 1:length(candidate_d), .combine = rbind) %dopar%{
   d<-candidate_d[j]
   results_cv <- matrix(0, nrow = 1, ncol = 4)
   colnames(results_cv) <- c("d","precision at 5","recall at 5","rmse")
   mean_precision_vec<-rep(0,folds)
   mean_recall_vec<-rep(0,folds)
   mean_rmse_vec<-rep(0,folds)
   for(i in 1:folds){
      train_set <- utility_matrix[splitfolds != i , ]
      valid_set <- utility_matrix[splitfolds == i, ]
      Vprime<-svd(train_set,nu=d,nv=d)$v
      precision_vector<-rep(0,nrow(valid_set))
      recall_vector<-rep(0,nrow(valid_set))
      rmse_vector<-rep(0,nrow(valid_set))
      for (userid in 1:nrow(valid_set)){
        user=valid_set[userid,]
        index_of_known<-which(user!=0)
        if(length(index_of_known)>k){
          takeout<-sample(1:length(index_of_known),size=k)
          stories_removed<-index_of_known[takeout]
          save_old<-user[stories_removed]
          user[stories_removed]<-0
          valid_set[userid,]<-user
          valid_u<-valid_set%*%Vprime
          recommended<-SVD_top_X_recommendations(valid_u,Vprime,userid,k,valid_set)
          matched=length(intersect(story_ids[stories_removed],recommended))
          precision_vector[userid]<-(matched/k)
          recall_vector[userid]<-(matched/(length(index_of_known)))
          user[stories_removed]<-save_old
          valid_set[userid,]<-user
          scores<-SVD_item_scores(valid_u,Vprime,userid)
          rmse<-sqrt(mean((scores-valid_set[userid,])^2))
          rmse_vector[userid]<-rmse
        }
      }
      mean_precision_vec[i] <- mean(precision_vector)
      mean_recall_vec[i]<-mean(recall_vector)
      mean_rmse_vec[i]<-mean(rmse_vector)
   }
   results_cv[1,]<-c(d,mean(mean_precision_vec),mean(mean_recall_vec),mean(mean_rmse_vec))
   return(results_cv)
}
})
parallel::stopCluster(cluster)

```

```{r}

data.frame(results)

ggplot(data=data.frame(results),aes(x=d,y=rmse))+
  geom_line()+
  xlab("Number of dimensions in SVD")+
  ylab("RMSE")

ggplot(data=data.frame(results),aes(x=d,y=precision.at.5))+
  geom_line()+
  xlab("Number of dimensions in SVD")+
  ylab("Precision at 5")

ggplot(data=data.frame(results),aes(x=d,y=recall.at.5))+
  geom_line()+
  xlab("Number of dimensions in SVD")+
  ylab("Recall at 5")
  
```
## Questions 

How many dimensions would you prefer to use in the SVD method?

Compare the performance of SVD method against collaborative filtering, and also compare the popularity of the items recommended by the SVD method versus the average popularity of the stories.

How would you modify these methods to recommend unpopular stories that don't receive much activity?

How would you make recommendations to a user with no history of activity?

Inspect the output of the various recommendation methods, and analyze the diversity of recommendations as you did in `rec_sys_tutorial`. Is the algorithm recommending stories with varying features and popularity?  