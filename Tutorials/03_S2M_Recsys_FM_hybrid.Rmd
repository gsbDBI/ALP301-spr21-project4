---
title: 'ALP 301: Stones2Milestones, Recommendation Systems (Factorization Machines and Ensembling)'
output:
  html_document:
    highlight: haddock
    number_sections: no
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '2'
  pdf_document:
    toc: yes
    toc_depth: '2'
date: "April 2021"
---

# Introduction

To review, there are two basic models of recommendation systems: content filtering, and collaborative filtering. Content filtering takes advantage of features of users and/or items, while collaborative filtering uses the matrix of interactions between users and items. 

In the previous tutorial, we tried out both of these basic models. At first, we tried to compare how the interaction history of each user or item are similar to other users/items; this falls in the category of collaborative filtering. Then, we experimented with content filtering by calculating similarity based on content observables. Finally, we tried SVD-based factorization methods, which again belongs to collaborative filtering, since we're using the matrix of interaction utilities. 

Is there a way to combine the power of both, utilizing content features and the utility matrix at the same time? Yes, and this class of methods is called Hybrid Filtering. 

In this tutorial, we first learn about a new, powerful technique called Factorization Machines, which has widespread applications in the industry. Then, we explore another technique called ensembling that basically takes a weighted average of basic models, to analyze its performance.


```{r setup, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse)
pacman::p_load(here)
pacman::p_load(lmtest)
pacman::p_load(glue)
pacman::p_load(broom)
pacman::p_load(ri2)
pacman::p_load(margins)
pacman::p_load(glmnet) 
pacman::p_load(kableExtra)
pacman::p_load(stargazer)
pacman::p_load(knitr)
pacman::p_load(doParallel)
pacman::p_load(corrplot)
pacman::p_load(corrplot)
pacman::p_load(rsparse)
pacman::p_load(recommenderlab)
pacman::p_load(reshape2)
pacman::p_load(ggplot2)

rm(list = ls())

# Note for 2021 data (coverage: 2020-05-01 to 2021-01-31): 
## filter60 means that we filtered out interactions involving
# children who made fewer than 60 interactions;
# after filtering, there are 11202 users left.
# This applies to both utils_mat_filtered.csv and utils_raw_filter60.csv.

utils_mat_file <- "Datasets/utils_mat_filtered.csv"
utils_raw_file <- "Datasets/utils_raw_filter60.csv"

# read story characteristics
story_info<- read_csv("Datasets/all_story_obs.csv")

# read utility matrix
utility_mat<- read_csv(utils_mat_file,col_types = cols(.default = col_double()))
utility_mat[is.na(utility_mat)]<-0.0

# read child-story interaction data
utility_data_raw<-read_csv(utils_raw_file,col_types = cols(.default = col_double()))

# number of columns in the utility_mat
num_cols <- 2527

child_ids<-as.integer(utility_mat$child_id_code)
story_ids<-as.integer(colnames(utility_mat[,3:num_cols]))

# only use stories with features available
stories_with_features<-(story_ids %in% story_info$story_id_code)

# discard first two columns of utility_mat (which are index and child_id_code)
utility_matrix<-utility_mat[,3:num_cols]

# only retain stories with features available to us
utility_matrix<-utility_matrix[,stories_with_features]

# story_ids; this is not sequential and thus different from column index numbers
story_ids<-colnames(utility_matrix)

# turn utility_matrix into R matrix format
utility_matrix<-as.matrix(utility_matrix)

rm(utility_mat)

```


# Factorization Machines: Combining the Utility Matrix with User Data

Here, we combine user and story features with the utility matrix to create a very large feature matrix and train a model to predict the ratings of a user.


```{r info}

# filter out N/A entries
utility_data_raw%>% filter(!is.na(intensity)) ->utility_data_raw

# select story characteristics to be used
story_info%>%select(one_of(c("totpage","wordcount","story_id_code")))->story_info

# combine story characteristics into the utility data
left_join(utility_data_raw,story_info, by = c("story_id_code"="story_id_code")) %>%drop_na->merged_data_all

# split train/test data
train_index<-sample(nrow(merged_data_all), floor(0.7*nrow(merged_data_all)))
train_set<-merged_data_all[train_index,]
test_set<-merged_data_all[-train_index,]
```

We will train a factorization machine model. The FM model takes in information about the user and item, and outputs the predicted utility (a single number between 0 and 1). Specifically, the input to the FM model include:

`story_id_code`: index for the story

`child_id_code`: index for the child

`totpage, wordcount`: story characteristics (refer to the last tutorial for their meaning)

Importantly, we can include content observables (i.e. `totpage` and `wordcount`) in the inputs.

We represent the data using a very large feature matrix. Each user-item interaction is a row in the data. Each user id and each story id is represented as a binary variable, which give us 11202 user id variables, and 1087 story id variables. To this, we add 2 story characteristics for each observation (`totpage` and `wordcount`). 

Below, we first load the data and transform it into sparse matrix format:

```{r load_data}
# we will feed the following 4 features into the FM model
formula_fm<- intensity ~ totpage+wordcount+story_id_code+child_id_code

train_matrix<-model.matrix(formula_fm, data = train_set)

#The below line makes the outcomes binary:
#if a user started a story, we code it as '1',
#and '0' otherwise. You can change it and
#keep the old encoding (0-0.3-0.5-1) by removing 
#the line below and changing the factorization
#machine function's family argument to 'gaussian'.
train_outcome<-ifelse(train_set$intensity>0.4,1,0)

train_matrix_sparse = as(train_matrix, "RsparseMatrix")
```

Then, we train the model without doing any cross validation for hyperparameters.

We take second order interactions of these terms, and we predict the rating of each user by using a lower dimensional representation of these features. We only use the non-missing entries in our utility matrix to train the model. We use a 70 percent-30 percent train/test split. You can have a look at the paper below for extra details and additional explanations of the workings of the FM model: https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf 

```{r fm}

# create factorization machine model and train it
fm = FactorizationMachine$new(learning_rate_w = 0.03, rank = 50, lambda_w = 0.001,
lambda_v = 0.001, family = "binomial", intercept = FALSE) 
system.time({
  res = fm$fit(train_matrix_sparse, train_outcome, n_iter = 200)
  })

# If you run the model with the usual (0-0.3-0.5-1) encoding,
#truncate the predictions of the model
#so that anything below 0 is truncated to 0,
#and anything above 1 is truncated to 1.
preds = fm$predict(train_matrix)

sqrt(mean((train_outcome-preds)^2))
```

Let's do cross validation to find a good low dimensional representation of this very high dimensional covariate matrix.
Try and pick how many dimensions you want to keep in the lower dimensional representation (the rank argument) and how much you want to regularize the linear terms (lambda_w) and the interaction terms (lambda_v). Compare the root mean squared error with the basic models below.

```{r, eval=FALSE}
# This will take a long time to run,
#so it is not run by default.
#Change to eval=TRUE to run it yourself.

cluster <- makeCluster(detectCores(logical = TRUE) - 1) 
registerDoParallel(cluster)

# load packages for each parallel session
clusterEvalQ(cluster, {
   library(rsparse)
  library(Matrix)
})
n<-nrow(train_matrix)
folds <- 3
splitfolds <- sample(1:folds, n, replace = TRUE)

# candidate hyperparameters for tuning: 
# (1) number of latents
candidate_rank<-c(25,50,100,200,300)
# (2) regularization param 1
candidate_lambda_w<-c(0.02,0.04,0.06)
# (3) regularization param 2
candidate_lambda_v<-c(0.02,0.04,0.06)

# iterate through the outer product of the three types of candidates
parameters<-cross3(candidate_rank,candidate_lambda_w,candidate_lambda_v)

# Export objects to the parallel sessions
clusterExport(cluster, c("train_matrix", "train_outcome","splitfolds","folds", "parameters"))

system.time({
results <- foreach(j = 1:length(parameters), .combine = rbind) %dopar%{
   pars<-unlist(parameters[j])
   rank_fm <- pars[1]
   lambda_w_fm<- pars[2]
   lambda_v_fm<-pars[3]
   results_cv <- matrix(0, nrow = 1, ncol = 4)
   colnames(results_cv) <- c("rank","lambda_w","lambda_v", "root MSE")
   meanrootMSE<-rep(0,folds)
   for(i in 1:folds){
      # set up training and evaluation sets
      cv_train_set <- train_matrix[splitfolds != i , ]
      cv_valid_set <- train_matrix[splitfolds == i, ]
      cv_train_set_outcome <- train_outcome[splitfolds != i ]
      cv_valid_set_outcome <- train_outcome[splitfolds == i]
      cv_train_set_sparse = as(cv_train_set, "RsparseMatrix")
      cv_valid_set_sparse = as(cv_valid_set, "RsparseMatrix")
      # create and train model with the hyperparameters
      fm = FactorizationMachine$new(learning_rate_w = 0.1, rank = rank_fm, lambda_w = lambda_w_fm,
      lambda_v = lambda_v_fm, family = "binomial", intercept = FALSE) 
      res = fm$fit(train_matrix_sparse, train_outcome, n_iter = 200)
      # make prediction and calculate error
      preds = fm$predict(cv_valid_set_sparse)
      meanrootMSE[i]<-sqrt(mean((cv_valid_set_outcome-preds)^2))
   }
   # record the results for this set of hyperparameters combo
   results_cv[1,]<-c(rank_fm, lambda_w_fm,lambda_v_fm, mean(meanrootMSE))
   return(results_cv)
}
})
stopCluster(cluster)

results_df <- as.data.frame(results)
results_df$minfreq<-as.character(results_df$minfreq)

```

FM model as built above is a predictive model: but we can use it as a recommendation algorithm by recommending the items with the highest predicted ratings for each user.

## Exercises

**What is the RMSE on the training set and the test set for the cross validated factorization machine model?(())

**Currently we used 2 story observables. Create a new dataframe by including user observables in the FM model as well. Use the user_info data frame. Compare the performance with the previous model with only story characteristics. Is there a substantial improvement in RMSE?**

# Ensemble: Combining basic models

In this section, we will create a recommender by combining (ensembling) several basic models. Ensembling simply means combining multiple models in some way (taking average of the outputs, or take the max of the outputs, etc.), and is a powerful machine learning technique; here we simply take weighted averages of different models. 

Note that we are not using content observables here, so the models in this section belong to collaborative filtering. Let's combine user and item-based collaborative filters first (you learned about these two basic models in the previous tutorial). 

```{r train_mat}
train_matrix<-utility_matrix
train_matrix <- as(train_matrix, "realRatingMatrix")
```

Let's compare their performance in terms of predicting ratings, using an 'all but one' approach.

```{r perf}
#Split our data: 70 percent for training and 30 percent for evaluation.
eval_scheme <- evaluationScheme(train_matrix, method="split", train=0.7, given=-1, goodRating=0.5)

# define combined collaborative recommender
combined_IB_UB <- HybridRecommender(
  Recommender(getData(eval_scheme, "train"), method = "IBCF"),
  Recommender(getData(eval_scheme, "train"), method = "UBCF"),
  weights = c(.5, .5)
  )

IBCF_model<-Recommender(getData(eval_scheme, "train"), method = "IBCF")

UBCF_model<-Recommender(getData(eval_scheme, "train"), method = "UBCF")

# make predictions and calculate errors
pred_IBCF<-predict(IBCF_model, getData(eval_scheme, "known"), type="ratings")
pred_UBCF<-predict(UBCF_model, getData(eval_scheme, "known"), type="ratings")
pred_combined<-predict(combined_IB_UB, getData(eval_scheme, "known"), type="ratings")

errors <- rbind(UBCF = calcPredictionAccuracy(pred_IBCF, getData(eval_scheme, "unknown")),IBCF=calcPredictionAccuracy(pred_UBCF, getData(eval_scheme, "unknown")), combined=calcPredictionAccuracy(pred_combined, getData(eval_scheme, "unknown")) )

errors

```


Let's plot the average rating per item in the predictions versus the training set for each algorithm.
The below plot is the distribution of the average rating of the recommended items.

```{r plt}
# plot average ratings for analysis
plot_frame <- data.frame(Overall=colMeans(train_matrix),IBCF=colMeans(pred_IBCF),UBCF=colMeans(pred_UBCF),combined=colMeans(pred_combined))

plot_frame<- melt(plot_frame)
ggplot(plot_frame,aes(x=value, fill=variable)) + geom_density(alpha=0.5)+xlim(c(0,1))+ xlab("Average Rating")

```

The recommender systems are still suggesting popular stories compared to the overall distribution. IBCF seems closer to the overall popularity distribution, while UBCF is further away and is heavily titled towards more popular stories. The combined method is somewhere in between. 

Next, add in randomness into out model and rerun the experiments.

```{r rdm}
# combined recommender that adds in randomness
random_IB_UB <- HybridRecommender(
  Recommender(getData(eval_scheme, "train"), method = "IBCF"),
  Recommender(getData(eval_scheme, "train"), method = "UBCF"),
  Recommender(getData(eval_scheme, "train"), method = "RANDOM"),
  weights = c(.4, .4, .2)
  )

# predict and calculate errors
pred_combined_random<-predict(random_IB_UB, getData(eval_scheme, "known"), type="ratings")

errors <- rbind(UBCF = calcPredictionAccuracy(pred_IBCF, getData(eval_scheme, "unknown")),IBCF=calcPredictionAccuracy(pred_UBCF, getData(eval_scheme, "unknown")), combined=calcPredictionAccuracy(pred_combined, getData(eval_scheme, "unknown")),combined_random=calcPredictionAccuracy(pred_combined_random, getData(eval_scheme, "unknown")) )

errors
```


```{r plt2}
# plot and analyze the distribution of ratings
plot_frame <- data.frame(Overall=colMeans(train_matrix),IBCF=colMeans(pred_IBCF),UBCF=colMeans(pred_UBCF),combined=colMeans(pred_combined),random=colMeans(pred_combined_random))

plot_frame<- melt(plot_frame)
ggplot(plot_frame,aes(x=value, fill=variable)) + geom_density(alpha=0.5)+xlim(c(0,1))+ xlab("Average Rating")


```

You can use the recommenderlab package to quickly create hybridizations of IBCF, UBCF, SVD, Random, and Most Popular recommendations.

## Exercises 

**Can we add some randomness to the recommendation models to create some diversity in outcomes? What do you think is a good mixture to create a combined model if we want to diversify and still maintain some accuracy? Build that model and make the case that it achieves these objectives. How did you choose the mixture weights?**

**Let's focus on low activity users. Try the 'Most Popular' and 'Random' recommendation models on users with low activity, and their mixtures with IBCF-UBCF models. Which one performs better? What would you use on users with no history?**

**At what level of activity do you think you have enough information to switch from random/most popular mixtures to standard IBCF/UBCF without randomness or most popular mixtures?**

**The above factorization machine model is run on the binary representation of the data, where we take the utility to be '1' if a user started a story, and 0 otherwise. Change the model (read the comments in line the code block `load_data`) to predict the ratings using the 0-0.3-0.5-1 utility intensity model we were using before. Does the performance change? How can you compare the two methods?**


